{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from mini_GPT import GPTLanguageModel, InputDataset, printlog, try_gpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "block_size = 256\n",
    "# max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = try_gpu()\n",
    "print(device)\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 制作字符级token数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "1115394 1 F\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Train and test splits\n",
    "print(len(text), len(text[0]), text[0])\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "train_dataset = InputDataset(train_data, block_size)\n",
    "val_dataset = InputDataset(val_data, block_size)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义/加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model = GPTLanguageModel(vocab_size, n_embd, n_head, n_layer, block_size, dropout, device)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./GPT_mini_model\\mini_gpt.pt\n",
      "Load trained model\n"
     ]
    }
   ],
   "source": [
    "ckpt_path_dir = './GPT_mini_model'\n",
    "if not os.path.exists(ckpt_path_dir):\n",
    "    os.makedirs(ckpt_path_dir)\n",
    "\n",
    "ckpt_path = os.path.join(ckpt_path_dir, 'mini_gpt.pt')\n",
    "print(ckpt_path)\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    print('Load trained model')\n",
    "    model.load_state_dict(torch.load(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0839, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader: \n",
    "    test_x, test_y = batch\n",
    "    break\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, loss_ = model(test_x, test_y)\n",
    "loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y, _, loss_ = None, None, None, None\n",
    "for i in range(100):\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_loss'\n",
    "mode = 'min'\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2023-05-10 21:55:12\n",
      "Epoch 0 / 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7841/7841 [1:12:02<00:00,  1.81it/s, train_loss=0.247]\n",
      "100%|██████████| 870/870 [02:28<00:00,  5.84it/s, val_loss=3.28]\n",
      "<<<<<< reach best val_loss: 3.2785569438989137 >>>>>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update param\n",
      "\n",
      "================================================================================2023-05-10 23:09:43\n",
      "Epoch 1 / 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7841/7841 [1:08:14<00:00,  1.91it/s, train_loss=0.223]\n",
      "100%|██████████| 870/870 [02:31<00:00,  5.75it/s, val_loss=3.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2023-05-11 00:20:30\n",
      "Epoch 2 / 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7841/7841 [1:06:01<00:00,  1.98it/s, train_loss=0.224]\n",
      "100%|██████████| 870/870 [02:05<00:00,  6.91it/s, val_loss=3.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2023-05-11 01:28:37\n",
      "Epoch 3 / 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7841/7841 [58:33<00:00,  2.23it/s, train_loss=0.224]\n",
      "100%|██████████| 870/870 [02:05<00:00,  6.93it/s, val_loss=3.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2023-05-11 02:29:16\n",
      "Epoch 4 / 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7841/7841 [58:24<00:00,  2.24it/s, train_loss=0.224]\n",
      "100%|██████████| 870/870 [02:05<00:00,  6.95it/s, val_loss=3.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2023-05-11 03:29:46\n",
      "Epoch 5 / 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7841/7841 [58:22<00:00,  2.24it/s, train_loss=0.224]\n",
      "100%|██████████| 870/870 [02:05<00:00,  6.95it/s, val_loss=3.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2023-05-11 04:30:14\n",
      "Epoch 6 / 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7841/7841 [58:26<00:00,  2.24it/s, train_loss=0.224]\n",
      "100%|██████████| 870/870 [02:05<00:00,  6.95it/s, val_loss=3.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2023-05-11 05:30:45\n",
      "Epoch 7 / 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7841/7841 [58:23<00:00,  2.24it/s, train_loss=0.224]\n",
      "100%|██████████| 870/870 [02:05<00:00,  6.94it/s, val_loss=3.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2023-05-11 06:31:14\n",
      "Epoch 8 / 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7841/7841 [58:23<00:00,  2.24it/s, train_loss=0.224]\n",
      "100%|██████████| 870/870 [02:05<00:00,  6.96it/s, val_loss=3.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2023-05-11 07:31:43\n",
      "Epoch 9 / 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7841/7841 [58:22<00:00,  2.24it/s, train_loss=0.224]\n",
      "100%|██████████| 870/870 [02:05<00:00,  6.94it/s, val_loss=3.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2023-05-11 08:32:10\n",
      "Epoch 10 / 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7841/7841 [58:36<00:00,  2.23it/s, train_loss=0.224]\n",
      "100%|██████████| 870/870 [02:04<00:00,  6.96it/s, val_loss=3.4] \n",
      "<<<<<< val_loss without improvement in 10 epoch, early stopping >>>>>>\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "history = {}\n",
    "for epoch in range(epochs):\n",
    "    printlog(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
    "    ## train\n",
    "    model.train()\n",
    "    total_loss, step = 0, 0\n",
    "    loop = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "    for i, batch in loop:\n",
    "        # if i > 1:\n",
    "        #     break\n",
    "        X, Y = batch\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        logits, loss = model(X, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        step_log = {'train_loss': loss.item()}\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        step += 1\n",
    "        if i != len(train_dataloader) - 1:\n",
    "            loop.set_postfix(**step_log)\n",
    "        else:\n",
    "            epoch_loss = total_loss / step\n",
    "            epoch_log = {'train_loss': epoch_loss}\n",
    "            loop.set_postfix(**epoch_log)\n",
    "    for name, metric in epoch_log.items():\n",
    "        history[name] = history.get(name, []) + [metric]\n",
    "    ## validation\n",
    "    model.eval()\n",
    "    total_loss, step = 0, 0\n",
    "    loop = tqdm(enumerate(val_dataloader), total=len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        for i, batch in loop:\n",
    "            # if i > 1:\n",
    "            #     break\n",
    "            X, Y = batch\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            preds, loss = model(X, Y)\n",
    "\n",
    "            step_log = {'val_loss': loss.item()}\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            step += 1\n",
    "            if i != len(val_dataloader) - 1:\n",
    "                loop.set_postfix(**step_log)\n",
    "            else:\n",
    "                epoch_loss = total_loss / step\n",
    "                epoch_log = {'val_loss': epoch_loss}\n",
    "                loop.set_postfix(**epoch_log)\n",
    "\n",
    "    epoch_log[\"epoch\"] = epoch\n",
    "    for name, metric in epoch_log.items():\n",
    "        history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "    ## early-stopping\n",
    "    arr_scores = history[monitor]\n",
    "    best_score_idx = np.argmin(arr_scores) if mode == 'min' else np.argmax(arr_scores)\n",
    "    # print(arr_scores, best_score_idx)\n",
    "    if best_score_idx == len(arr_scores) - 1:\n",
    "        print('update param')\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"<<<<<< reach best {monitor}: {arr_scores[best_score_idx]} >>>>>>\", file=sys.stderr)\n",
    "    if len(arr_scores) - best_score_idx > patience:\n",
    "        print(f\"<<<<<< {monitor} without improvement in {patience} epoch, early stopping >>>>>>\", file=sys.stderr)\n",
    "        break\n",
    "\n",
    "    model.load_state_dict(torch.load(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 train_loss、val_loss\n",
    "dfhistory = pd.DataFrame(history)\n",
    "dfhistory.to_csv('./mini_gpt_train_record.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT生成文本"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 直接观察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "for batch in val_dataloader:\n",
    "    context, _ = batch\n",
    "    break\n",
    "context = context.to(device)\n",
    "\n",
    "pred_step = 50\n",
    "print(pred_step)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(context, max_new_tokens=pred_step)\n",
    "res = [decode(out[i].tolist()) for i in range(len(out))]\n",
    "out = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "true_res = [decode(data[n + block_size + i : n + block_size + i + pred_step].tolist()) for i in range(len(context))]\n",
    "print(len(true_res[0]), type(true_res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 \n",
      " rina that:\n",
      "But a word with you of me?\n",
      "\n",
      "PARIS:\n",
      "Thes\n",
      "\n",
      "======================================\n",
      "\n",
      "50 \n",
      " rina.\n",
      "\n",
      "GREMIO:\n",
      "You are too blunt: go to it orderly\n"
     ]
    }
   ],
   "source": [
    "print(len(res[0][block_size:]), '\\n', res[0][block_size:])\n",
    "print('\\n======================================\\n')\n",
    "print(len(true_res[0]), '\\n', true_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将生成结果写入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_res(pred_step, pred_times, out_file, dl, train_data=False):\n",
    "    fw = open(out_file, 'w')\n",
    "    for i, batch in enumerate(dl):\n",
    "        if i == pred_times:\n",
    "            break\n",
    "        context, _ = batch\n",
    "        context = context[0].reshape([1, -1])\n",
    "        context = context.to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            out = model.generate(context, max_new_tokens=pred_step)\n",
    "        res = decode(out[0].cpu().tolist())\n",
    "        if train_data:\n",
    "            true_res = decode(data[block_size + i * batch_size : block_size + i * batch_size + pred_step].tolist())\n",
    "        else:\n",
    "            true_res = decode(data[n + block_size + i * batch_size : n + block_size + i * batch_size + pred_step].tolist())\n",
    "\n",
    "        fw.write('#'*80)\n",
    "        fw.write('\\ninput text:\\n')\n",
    "        fw.write(res[:block_size])\n",
    "        fw.write('\\n' + '-'*60 + '\\n')\n",
    "        fw.write('pred text:\\n')\n",
    "        fw.write(res[block_size:])\n",
    "        fw.write('\\n' + '-'*60 + '\\n')\n",
    "        fw.write('label text:\\n')\n",
    "        fw.write(true_res)\n",
    "        fw.write('\\n'+'#'*80+'\\n\\n')\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_step = 50\n",
    "pred_times = 10\n",
    "\n",
    "train_pred_res = 'train_pred_text.txt'\n",
    "val_pred_res = 'val_pred_text.txt'\n",
    "\n",
    "get_test_res(pred_step, pred_times, train_pred_res, DataLoader(train_dataset, batch_size=batch_size, shuffle=False), True)\n",
    "get_test_res(pred_step, pred_times, val_pred_res, val_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_pred_text.txt 的第一组结果"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "################################################################################\n",
    "input text:\n",
    "First Citizen:\n",
    "Before we proceed any further, hear me speak.\n",
    "\n",
    "All:\n",
    "Speak, speak.\n",
    "\n",
    "First Citizen:\n",
    "You are all resolved rather to die than to famish?\n",
    "\n",
    "All:\n",
    "Resolved. resolved.\n",
    "\n",
    "First Citizen:\n",
    "First, you know Caius Marcius is chief enemy to the people.\n",
    "\n",
    "All:\n",
    "\n",
    "------------------------------------------------------------\n",
    "pred text:\n",
    "We know't, we know't.\n",
    "\n",
    "First Citizen:\n",
    "Let us kill \n",
    "------------------------------------------------------------\n",
    "label text:\n",
    "We know't, we know't.\n",
    "\n",
    "First Citizen:\n",
    "Let us kill \n",
    "################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_pred_text.txt 的第一组结果"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "################################################################################\n",
    "input text:\n",
    "?\n",
    "\n",
    "GREMIO:\n",
    "Good morrow, neighbour Baptista.\n",
    "\n",
    "BAPTISTA:\n",
    "Good morrow, neighbour Gremio.\n",
    "God save you, gentlemen!\n",
    "\n",
    "PETRUCHIO:\n",
    "And you, good sir! Pray, have you not a daughter\n",
    "Call'd Katharina, fair and virtuous?\n",
    "\n",
    "BAPTISTA:\n",
    "I have a daughter, sir, called Katha\n",
    "------------------------------------------------------------\n",
    "pred text:\n",
    "rina that:\n",
    "Believe I have consider'd you with a fe\n",
    "------------------------------------------------------------\n",
    "label text:\n",
    "rina.\n",
    "\n",
    "GREMIO:\n",
    "You are too blunt: go to it orderly\n",
    "################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-gpu",
   "language": "python",
   "name": "pt-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
