{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from mini_GPT import GPTLanguageModel, InputDataset, printlog, try_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "block_size = 256\n",
    "# max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = try_gpu()\n",
    "print(device)\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "1115394 1 F\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Train and test splits\n",
    "print(len(text), len(text[0]), text[0])\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "train_dataset = InputDataset(train_data, block_size)\n",
    "val_dataset = InputDataset(val_data, block_size)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model = GPTLanguageModel(vocab_size, n_embd, n_head, n_layer, block_size, dropout, device)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./GPT_mini_model\\mini_gpt.pt\n",
      "Load trained model\n"
     ]
    }
   ],
   "source": [
    "ckpt_path_dir = './GPT_mini_model'\n",
    "if not os.path.exists(ckpt_path_dir):\n",
    "    os.makedirs(ckpt_path_dir)\n",
    "\n",
    "ckpt_path = os.path.join(ckpt_path_dir, 'mini_gpt.pt')\n",
    "print(ckpt_path)\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    print('Load trained model')\n",
    "    model.load_state_dict(torch.load(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0905, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader: \n",
    "    test_x, test_y = batch\n",
    "    break\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, loss_ = model(test_x, test_y)\n",
    "loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y, _, loss_ = None, None, None, None\n",
    "for i in range(100):\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_loss'\n",
    "mode = 'min'\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2023-05-10 21:55:12\n",
      "Epoch 0 / 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 5609/7841 [53:09<19:12,  1.94it/s, train_loss=0.234]  "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "history = {}\n",
    "for epoch in range(epochs):\n",
    "    printlog(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
    "    ## train\n",
    "    model.train()\n",
    "    total_loss, step = 0, 0\n",
    "    loop = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "    for i, batch in loop:\n",
    "        # if i > 1:\n",
    "        #     break\n",
    "        X, Y = batch\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        logits, loss = model(X, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        step_log = {'train_loss': loss.item()}\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        step += 1\n",
    "        if i != len(train_dataloader) - 1:\n",
    "            loop.set_postfix(**step_log)\n",
    "        else:\n",
    "            epoch_loss = total_loss / step\n",
    "            epoch_log = {'train_loss': epoch_loss}\n",
    "            loop.set_postfix(**epoch_log)\n",
    "    for name, metric in epoch_log.items():\n",
    "        history[name] = history.get(name, []) + [metric]\n",
    "    ## validation\n",
    "    model.eval()\n",
    "    total_loss, step = 0, 0\n",
    "    loop = tqdm(enumerate(val_dataloader), total=len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        for i, batch in loop:\n",
    "            # if i > 1:\n",
    "            #     break\n",
    "            X, Y = batch\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            preds, loss = model(X, Y)\n",
    "\n",
    "            step_log = {'val_loss': loss.item()}\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            step += 1\n",
    "            if i != len(val_dataloader) - 1:\n",
    "                loop.set_postfix(**step_log)\n",
    "            else:\n",
    "                epoch_loss = total_loss / step\n",
    "                epoch_log = {'val_loss': epoch_loss}\n",
    "                loop.set_postfix(**epoch_log)\n",
    "\n",
    "    epoch_log[\"epoch\"] = epoch\n",
    "    for name, metric in epoch_log.items():\n",
    "        history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "    ## early-stopping\n",
    "    arr_scores = history[monitor]\n",
    "    best_score_idx = np.argmin(arr_scores) if mode == 'min' else np.argmax(arr_scores)\n",
    "    # print(arr_scores, best_score_idx)\n",
    "    if best_score_idx == len(arr_scores) - 1:\n",
    "        print('update param')\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"<<<<<< reach best {monitor}: {arr_scores[best_score_idx]} >>>>>>\", file=sys.stderr)\n",
    "    if len(arr_scores) - best_score_idx > patience:\n",
    "        print(f\"<<<<<< {monitor} without improvement in {patience} epoch, early stopping >>>>>>\", file=sys.stderr)\n",
    "        break\n",
    "\n",
    "    model.load_state_dict(torch.load(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhistory = pd.DataFrame(history)\n",
    "dfhistory.to_csv('./mini_gpt_train_record.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "for batch in val_dataloader:\n",
    "    context, _ = batch\n",
    "    break\n",
    "context = context.to(device)\n",
    "\n",
    "pred_step = 50\n",
    "print(pred_step)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(context, max_new_tokens=pred_step)\n",
    "res = [decode(out[i].tolist()) for i in range(len(out))]\n",
    "out = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "true_res = [decode(data[n + block_size + i : n + block_size + i + pred_step].tolist()) for i in range(len(context))]\n",
    "print(len(true_res[0]), type(true_res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 \n",
      " rina,\n",
      "know your men remain and my heart of them,\n",
      "t\n",
      "\n",
      "======================================\n",
      "\n",
      "50 \n",
      " rina.\n",
      "\n",
      "GREMIO:\n",
      "You are too blunt: go to it orderly\n"
     ]
    }
   ],
   "source": [
    "print(len(res[0][block_size:]), '\\n', res[0][block_size:])\n",
    "print('\\n======================================\\n')\n",
    "print(len(true_res[0]), '\\n', true_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-gpu",
   "language": "python",
   "name": "pt-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
